{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/shakespeare.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'I')\n",
      "(1, ';')\n",
      "(2, '6')\n",
      "(3, '1')\n",
      "(4, '4')\n",
      "(5, 'p')\n",
      "(6, '`')\n",
      "(7, 'A')\n",
      "(8, '-')\n",
      "(9, '?')\n",
      "(10, '&')\n",
      "(11, '9')\n",
      "(12, '2')\n",
      "(13, 'K')\n",
      "(14, 'j')\n",
      "(15, ']')\n",
      "(16, 'Y')\n",
      "(17, 'D')\n",
      "(18, 'L')\n",
      "(19, ',')\n",
      "(20, '\\n')\n",
      "(21, 'F')\n",
      "(22, 'G')\n",
      "(23, '!')\n",
      "(24, 'Z')\n",
      "(25, '[')\n",
      "(26, '.')\n",
      "(27, 'C')\n",
      "(28, 'R')\n",
      "(29, 's')\n",
      "(30, ' ')\n",
      "(31, 't')\n",
      "(32, '(')\n",
      "(33, 'P')\n",
      "(34, ')')\n",
      "(35, 'w')\n",
      "(36, 'T')\n",
      "(37, 'S')\n",
      "(38, 'z')\n",
      "(39, '<')\n",
      "(40, '3')\n",
      "(41, '5')\n",
      "(42, 'c')\n",
      "(43, '|')\n",
      "(44, 'N')\n",
      "(45, 'n')\n",
      "(46, 'f')\n",
      "(47, 'E')\n",
      "(48, 'v')\n",
      "(49, 'r')\n",
      "(50, 'a')\n",
      "(51, 'J')\n",
      "(52, 'X')\n",
      "(53, 'g')\n",
      "(54, 'Q')\n",
      "(55, '>')\n",
      "(56, 'd')\n",
      "(57, 'h')\n",
      "(58, 'W')\n",
      "(59, '0')\n",
      "(60, '8')\n",
      "(61, 'M')\n",
      "(62, 'i')\n",
      "(63, 'y')\n",
      "(64, ':')\n",
      "(65, 'x')\n",
      "(66, '}')\n",
      "(67, 'u')\n",
      "(68, 'B')\n",
      "(69, 'H')\n",
      "(70, '_')\n",
      "(71, 'b')\n",
      "(72, 'e')\n",
      "(73, 'q')\n",
      "(74, 'o')\n",
      "(75, 'V')\n",
      "(76, 'k')\n",
      "(77, \"'\")\n",
      "(78, '7')\n",
      "(79, '\"')\n",
      "(80, 'U')\n",
      "(81, 'l')\n",
      "(82, 'O')\n",
      "(83, 'm')\n"
     ]
    }
   ],
   "source": [
    "for pair in enumerate(all_characters):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "# num --> letter\n",
    "\n",
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "# letter --> num\n",
    "\n",
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0,\n",
       " ';': 1,\n",
       " '6': 2,\n",
       " '1': 3,\n",
       " '4': 4,\n",
       " 'p': 5,\n",
       " '`': 6,\n",
       " 'A': 7,\n",
       " '-': 8,\n",
       " '?': 9,\n",
       " '&': 10,\n",
       " '9': 11,\n",
       " '2': 12,\n",
       " 'K': 13,\n",
       " 'j': 14,\n",
       " ']': 15,\n",
       " 'Y': 16,\n",
       " 'D': 17,\n",
       " 'L': 18,\n",
       " ',': 19,\n",
       " '\\n': 20,\n",
       " 'F': 21,\n",
       " 'G': 22,\n",
       " '!': 23,\n",
       " 'Z': 24,\n",
       " '[': 25,\n",
       " '.': 26,\n",
       " 'C': 27,\n",
       " 'R': 28,\n",
       " 's': 29,\n",
       " ' ': 30,\n",
       " 't': 31,\n",
       " '(': 32,\n",
       " 'P': 33,\n",
       " ')': 34,\n",
       " 'w': 35,\n",
       " 'T': 36,\n",
       " 'S': 37,\n",
       " 'z': 38,\n",
       " '<': 39,\n",
       " '3': 40,\n",
       " '5': 41,\n",
       " 'c': 42,\n",
       " '|': 43,\n",
       " 'N': 44,\n",
       " 'n': 45,\n",
       " 'f': 46,\n",
       " 'E': 47,\n",
       " 'v': 48,\n",
       " 'r': 49,\n",
       " 'a': 50,\n",
       " 'J': 51,\n",
       " 'X': 52,\n",
       " 'g': 53,\n",
       " 'Q': 54,\n",
       " '>': 55,\n",
       " 'd': 56,\n",
       " 'h': 57,\n",
       " 'W': 58,\n",
       " '0': 59,\n",
       " '8': 60,\n",
       " 'M': 61,\n",
       " 'i': 62,\n",
       " 'y': 63,\n",
       " ':': 64,\n",
       " 'x': 65,\n",
       " '}': 66,\n",
       " 'u': 67,\n",
       " 'B': 68,\n",
       " 'H': 69,\n",
       " '_': 70,\n",
       " 'b': 71,\n",
       " 'e': 72,\n",
       " 'q': 73,\n",
       " 'o': 74,\n",
       " 'V': 75,\n",
       " 'k': 76,\n",
       " \"'\": 77,\n",
       " '7': 78,\n",
       " '\"': 79,\n",
       " 'U': 80,\n",
       " 'l': 81,\n",
       " 'O': 82,\n",
       " 'm': 83}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "       30, 30, 30, 30, 30,  3, 20, 30, 30, 21, 49, 74, 83, 30, 46, 50, 62,\n",
       "       49, 72, 29, 31, 30, 42, 49, 72, 50, 31, 67, 49, 72, 29, 30, 35, 72,\n",
       "       30, 56, 72, 29, 62, 49, 72, 30, 62, 45, 42, 49, 72, 50, 29, 72, 19,\n",
       "       20, 30, 30, 36, 57, 50, 31, 30, 31, 57, 72, 49, 72, 71, 63, 30, 71,\n",
       "       72, 50, 67, 31, 63, 77, 29, 30, 49, 74, 29, 72, 30, 83, 62, 53, 57,\n",
       "       31, 30, 45, 72, 48, 72, 49, 30, 56, 62, 72, 19, 20, 30, 30, 68, 67,\n",
       "       31, 30, 50, 29, 30, 31, 57, 72, 30, 49, 62,  5, 72, 49, 30, 29, 57,\n",
       "       74, 67, 81, 56, 30, 71, 63, 30, 31, 62, 83, 72, 30, 56, 72, 42, 72,\n",
       "       50, 29, 72, 19, 20, 30, 30, 69, 62, 29, 30, 31, 72, 45, 56, 72, 49,\n",
       "       30, 57, 72, 62, 49, 30, 83, 62, 53, 57, 31, 30, 71, 72, 50, 49, 30,\n",
       "       57, 62, 29, 30, 83, 72, 83, 74, 49, 63, 64, 20, 30, 30, 68, 67, 31,\n",
       "       30, 31, 57, 74, 67, 30, 42, 74, 45, 31, 49, 50, 42, 31, 72, 56, 30,\n",
       "       31, 74, 30, 31, 57, 62, 45, 72, 30, 74, 35, 45, 30, 71, 49, 62, 53,\n",
       "       57, 31, 30, 72, 63, 72, 29, 19, 20, 30, 30, 21, 72, 72, 56, 77, 29,\n",
       "       31, 30, 31, 57, 63, 30, 81, 62, 53, 57, 31, 77, 29, 30, 46, 81, 50,\n",
       "       83, 72, 30, 35, 62, 31, 57, 30, 29, 72, 81, 46,  8, 29, 67, 71, 29,\n",
       "       31, 50, 45, 31, 62, 50, 81, 30, 46, 67, 72, 81, 19, 20, 30, 30, 61,\n",
       "       50, 76, 62, 45, 53, 30, 50, 30, 46, 50, 83, 62, 45, 72, 30, 35, 57,\n",
       "       72, 49, 72, 30, 50, 71, 67, 45, 56, 50, 45, 42, 72, 30, 81, 62, 72,\n",
       "       29, 19, 20, 30, 30, 36, 57, 63, 30, 29, 72, 81, 46, 30, 31, 57, 63,\n",
       "       30, 46, 74, 72, 19, 30, 31, 74, 30, 31, 57, 63, 30, 29, 35, 72, 72,\n",
       "       31, 30, 29, 72, 81, 46, 30, 31, 74, 74, 30, 42, 49, 67, 72, 81, 64,\n",
       "       20, 30, 30, 36, 57, 74, 67, 30, 31])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    #encoded_text --> batch of encoded text\n",
    "    #num of unique characters --> len(set(text))\n",
    "    \n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What batches I want now?\n",
    "\n",
    "X = [H,e,l,l,o]\n",
    "Y [e,l,l,o, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "       30, 30, 30])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2, seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 30, 30, 30, 30],\n",
       "       [30, 30, 30, 30, 30]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30, 30, 30, 30, 30],\n",
       "       [30, 30, 30, 30, 30]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=True):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fcl = nn.Linear(num_hidden, len(self.all_chars))\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        final_output = self.fcl(drop_output)\n",
    "        \n",
    "        return final_output, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                      torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
    "            \n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
    "                      torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(all_chars=all_characters,\n",
    "                 num_hidden=256,\n",
    "                 num_layers=4,\n",
    "                 drop_prob=0.5,\n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86016\n",
      "262144\n",
      "1024\n",
      "1024\n",
      "262144\n",
      "262144\n",
      "1024\n",
      "1024\n",
      "262144\n",
      "262144\n",
      "1024\n",
      "1024\n",
      "262144\n",
      "262144\n",
      "1024\n",
      "1024\n",
      "21504\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "total_params = []\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param.numel())\n",
    "    total_params.append(int(param.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950804"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "valid_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544560"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901049"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text) * train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "valid_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901048"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544561"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "batch_size = 128\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "tracker = 0\n",
    "\n",
    "num_char = max(encoded_text) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 25 Valid loss: 3.2179415225982666\n",
      "Epoch: 1 Step: 50 Valid loss: 3.212709426879883\n",
      "Epoch: 1 Step: 75 Valid loss: 3.211197853088379\n",
      "Epoch: 1 Step: 100 Valid loss: 3.210162401199341\n",
      "Epoch: 1 Step: 125 Valid loss: 3.2096402645111084\n",
      "Epoch: 1 Step: 150 Valid loss: 3.2094571590423584\n",
      "Epoch: 1 Step: 175 Valid loss: 3.2092034816741943\n",
      "Epoch: 1 Step: 200 Valid loss: 3.2099928855895996\n",
      "Epoch: 1 Step: 225 Valid loss: 3.209319829940796\n",
      "Epoch: 1 Step: 250 Valid loss: 3.210076093673706\n",
      "Epoch: 1 Step: 275 Valid loss: 3.208062171936035\n",
      "Epoch: 1 Step: 300 Valid loss: 3.2094366550445557\n",
      "Epoch: 1 Step: 325 Valid loss: 3.207040309906006\n",
      "Epoch: 1 Step: 350 Valid loss: 3.208120346069336\n",
      "Epoch: 1 Step: 375 Valid loss: 3.207515001296997\n",
      "Epoch: 2 Step: 400 Valid loss: 3.208292245864868\n",
      "Epoch: 2 Step: 425 Valid loss: 3.2094566822052\n",
      "Epoch: 2 Step: 450 Valid loss: 3.209390878677368\n",
      "Epoch: 2 Step: 475 Valid loss: 3.207761764526367\n",
      "Epoch: 2 Step: 500 Valid loss: 3.2016968727111816\n",
      "Epoch: 2 Step: 525 Valid loss: 3.116615056991577\n",
      "Epoch: 2 Step: 550 Valid loss: 3.0203182697296143\n",
      "Epoch: 2 Step: 575 Valid loss: 2.8144872188568115\n",
      "Epoch: 2 Step: 600 Valid loss: 2.6464035511016846\n",
      "Epoch: 2 Step: 625 Valid loss: 2.539839744567871\n",
      "Epoch: 2 Step: 650 Valid loss: 2.470621109008789\n",
      "Epoch: 2 Step: 675 Valid loss: 2.424506425857544\n",
      "Epoch: 2 Step: 700 Valid loss: 2.3857288360595703\n",
      "Epoch: 2 Step: 725 Valid loss: 2.3497769832611084\n",
      "Epoch: 2 Step: 750 Valid loss: 2.3203208446502686\n",
      "Epoch: 3 Step: 775 Valid loss: 2.2893118858337402\n",
      "Epoch: 3 Step: 800 Valid loss: 2.262547254562378\n",
      "Epoch: 3 Step: 825 Valid loss: 2.233203172683716\n",
      "Epoch: 3 Step: 850 Valid loss: 2.2101008892059326\n",
      "Epoch: 3 Step: 875 Valid loss: 2.1884913444519043\n",
      "Epoch: 3 Step: 900 Valid loss: 2.162505865097046\n",
      "Epoch: 3 Step: 925 Valid loss: 2.142866373062134\n",
      "Epoch: 3 Step: 950 Valid loss: 2.1255109310150146\n",
      "Epoch: 3 Step: 975 Valid loss: 2.1085166931152344\n",
      "Epoch: 3 Step: 1000 Valid loss: 2.091254949569702\n",
      "Epoch: 3 Step: 1025 Valid loss: 2.0764849185943604\n",
      "Epoch: 3 Step: 1050 Valid loss: 2.0629467964172363\n",
      "Epoch: 3 Step: 1075 Valid loss: 2.0478157997131348\n",
      "Epoch: 3 Step: 1100 Valid loss: 2.035092353820801\n",
      "Epoch: 3 Step: 1125 Valid loss: 2.025230646133423\n",
      "Epoch: 4 Step: 1150 Valid loss: 2.005767345428467\n",
      "Epoch: 4 Step: 1175 Valid loss: 1.9978256225585938\n",
      "Epoch: 4 Step: 1200 Valid loss: 1.9806920289993286\n",
      "Epoch: 4 Step: 1225 Valid loss: 1.971278429031372\n",
      "Epoch: 4 Step: 1250 Valid loss: 1.956629991531372\n",
      "Epoch: 4 Step: 1275 Valid loss: 1.945387601852417\n",
      "Epoch: 4 Step: 1300 Valid loss: 1.9313336610794067\n",
      "Epoch: 4 Step: 1325 Valid loss: 1.9221910238265991\n",
      "Epoch: 4 Step: 1350 Valid loss: 1.9093451499938965\n",
      "Epoch: 4 Step: 1375 Valid loss: 1.9028393030166626\n",
      "Epoch: 4 Step: 1400 Valid loss: 1.8927932977676392\n",
      "Epoch: 4 Step: 1425 Valid loss: 1.882309913635254\n",
      "Epoch: 4 Step: 1450 Valid loss: 1.87080717086792\n",
      "Epoch: 4 Step: 1475 Valid loss: 1.8662818670272827\n",
      "Epoch: 4 Step: 1500 Valid loss: 1.8580148220062256\n",
      "Epoch: 4 Step: 1525 Valid loss: 1.8554309606552124\n",
      "Epoch: 5 Step: 1550 Valid loss: 1.8379549980163574\n",
      "Epoch: 5 Step: 1575 Valid loss: 1.8282235860824585\n",
      "Epoch: 5 Step: 1600 Valid loss: 1.820717453956604\n",
      "Epoch: 5 Step: 1625 Valid loss: 1.8139240741729736\n",
      "Epoch: 5 Step: 1650 Valid loss: 1.805747628211975\n",
      "Epoch: 5 Step: 1675 Valid loss: 1.797971248626709\n",
      "Epoch: 5 Step: 1700 Valid loss: 1.7934101819992065\n",
      "Epoch: 5 Step: 1725 Valid loss: 1.7860312461853027\n",
      "Epoch: 5 Step: 1750 Valid loss: 1.7783803939819336\n",
      "Epoch: 5 Step: 1775 Valid loss: 1.7699967622756958\n",
      "Epoch: 5 Step: 1800 Valid loss: 1.7635215520858765\n",
      "Epoch: 5 Step: 1825 Valid loss: 1.7606550455093384\n",
      "Epoch: 5 Step: 1850 Valid loss: 1.7519803047180176\n",
      "Epoch: 5 Step: 1875 Valid loss: 1.7481342554092407\n",
      "Epoch: 5 Step: 1900 Valid loss: 1.7428044080734253\n",
      "Epoch: 6 Step: 1925 Valid loss: 1.7359918355941772\n",
      "Epoch: 6 Step: 1950 Valid loss: 1.7337535619735718\n",
      "Epoch: 6 Step: 1975 Valid loss: 1.7270610332489014\n",
      "Epoch: 6 Step: 2000 Valid loss: 1.7205075025558472\n",
      "Epoch: 6 Step: 2025 Valid loss: 1.7131177186965942\n",
      "Epoch: 6 Step: 2050 Valid loss: 1.7122300863265991\n",
      "Epoch: 6 Step: 2075 Valid loss: 1.702569603919983\n",
      "Epoch: 6 Step: 2100 Valid loss: 1.7005093097686768\n",
      "Epoch: 6 Step: 2125 Valid loss: 1.6897510290145874\n",
      "Epoch: 6 Step: 2150 Valid loss: 1.6878715753555298\n",
      "Epoch: 6 Step: 2175 Valid loss: 1.6855517625808716\n",
      "Epoch: 6 Step: 2200 Valid loss: 1.6814237833023071\n",
      "Epoch: 6 Step: 2225 Valid loss: 1.673721432685852\n",
      "Epoch: 6 Step: 2250 Valid loss: 1.6694793701171875\n",
      "Epoch: 6 Step: 2275 Valid loss: 1.6666100025177002\n",
      "Epoch: 7 Step: 2300 Valid loss: 1.6622062921524048\n",
      "Epoch: 7 Step: 2325 Valid loss: 1.6602648496627808\n",
      "Epoch: 7 Step: 2350 Valid loss: 1.6537836790084839\n",
      "Epoch: 7 Step: 2375 Valid loss: 1.650566816329956\n",
      "Epoch: 7 Step: 2400 Valid loss: 1.6454826593399048\n",
      "Epoch: 7 Step: 2425 Valid loss: 1.6415122747421265\n",
      "Epoch: 7 Step: 2450 Valid loss: 1.6359738111495972\n",
      "Epoch: 7 Step: 2475 Valid loss: 1.631558895111084\n",
      "Epoch: 7 Step: 2500 Valid loss: 1.6284832954406738\n",
      "Epoch: 7 Step: 2525 Valid loss: 1.6256005764007568\n",
      "Epoch: 7 Step: 2550 Valid loss: 1.6232621669769287\n",
      "Epoch: 7 Step: 2575 Valid loss: 1.61545729637146\n",
      "Epoch: 7 Step: 2600 Valid loss: 1.6152092218399048\n",
      "Epoch: 7 Step: 2625 Valid loss: 1.6076185703277588\n",
      "Epoch: 7 Step: 2650 Valid loss: 1.6086702346801758\n",
      "Epoch: 8 Step: 2675 Valid loss: 1.602197289466858\n",
      "Epoch: 8 Step: 2700 Valid loss: 1.6025750637054443\n",
      "Epoch: 8 Step: 2725 Valid loss: 1.5970234870910645\n",
      "Epoch: 8 Step: 2750 Valid loss: 1.596093773841858\n",
      "Epoch: 8 Step: 2775 Valid loss: 1.589754581451416\n",
      "Epoch: 8 Step: 2800 Valid loss: 1.586991548538208\n",
      "Epoch: 8 Step: 2825 Valid loss: 1.581820011138916\n",
      "Epoch: 8 Step: 2850 Valid loss: 1.5815422534942627\n",
      "Epoch: 8 Step: 2875 Valid loss: 1.5803637504577637\n",
      "Epoch: 8 Step: 2900 Valid loss: 1.5794988870620728\n",
      "Epoch: 8 Step: 2925 Valid loss: 1.574465274810791\n",
      "Epoch: 8 Step: 2950 Valid loss: 1.5722094774246216\n",
      "Epoch: 8 Step: 2975 Valid loss: 1.5683561563491821\n",
      "Epoch: 8 Step: 3000 Valid loss: 1.5678712129592896\n",
      "Epoch: 8 Step: 3025 Valid loss: 1.5656434297561646\n",
      "Epoch: 8 Step: 3050 Valid loss: 1.5604124069213867\n",
      "Epoch: 9 Step: 3075 Valid loss: 1.560958981513977\n",
      "Epoch: 9 Step: 3100 Valid loss: 1.5590275526046753\n",
      "Epoch: 9 Step: 3125 Valid loss: 1.558846116065979\n",
      "Epoch: 9 Step: 3150 Valid loss: 1.5534446239471436\n",
      "Epoch: 9 Step: 3175 Valid loss: 1.5506466627120972\n",
      "Epoch: 9 Step: 3200 Valid loss: 1.5530915260314941\n",
      "Epoch: 9 Step: 3225 Valid loss: 1.548789620399475\n",
      "Epoch: 9 Step: 3250 Valid loss: 1.5479811429977417\n",
      "Epoch: 9 Step: 3275 Valid loss: 1.5422688722610474\n",
      "Epoch: 9 Step: 3300 Valid loss: 1.5415016412734985\n",
      "Epoch: 9 Step: 3325 Valid loss: 1.5429986715316772\n",
      "Epoch: 9 Step: 3350 Valid loss: 1.536556363105774\n",
      "Epoch: 9 Step: 3375 Valid loss: 1.5297455787658691\n",
      "Epoch: 9 Step: 3400 Valid loss: 1.5356764793395996\n",
      "Epoch: 9 Step: 3425 Valid loss: 1.5355181694030762\n",
      "Epoch: 10 Step: 3450 Valid loss: 1.5310227870941162\n",
      "Epoch: 10 Step: 3475 Valid loss: 1.5309921503067017\n",
      "Epoch: 10 Step: 3500 Valid loss: 1.532294511795044\n",
      "Epoch: 10 Step: 3525 Valid loss: 1.5281472206115723\n",
      "Epoch: 10 Step: 3550 Valid loss: 1.5243144035339355\n",
      "Epoch: 10 Step: 3575 Valid loss: 1.526155710220337\n",
      "Epoch: 10 Step: 3600 Valid loss: 1.5219496488571167\n",
      "Epoch: 10 Step: 3625 Valid loss: 1.5198419094085693\n",
      "Epoch: 10 Step: 3650 Valid loss: 1.524459719657898\n",
      "Epoch: 10 Step: 3675 Valid loss: 1.5193241834640503\n",
      "Epoch: 10 Step: 3700 Valid loss: 1.5179492235183716\n",
      "Epoch: 10 Step: 3725 Valid loss: 1.513961911201477\n",
      "Epoch: 10 Step: 3750 Valid loss: 1.513342261314392\n",
      "Epoch: 10 Step: 3775 Valid loss: 1.512694239616394\n",
      "Epoch: 10 Step: 3800 Valid loss: 1.5162304639816284\n",
      "Epoch: 11 Step: 3825 Valid loss: 1.5158202648162842\n",
      "Epoch: 11 Step: 3850 Valid loss: 1.512441873550415\n",
      "Epoch: 11 Step: 3875 Valid loss: 1.5119112730026245\n",
      "Epoch: 11 Step: 3900 Valid loss: 1.5099270343780518\n",
      "Epoch: 11 Step: 3925 Valid loss: 1.5066441297531128\n",
      "Epoch: 11 Step: 3950 Valid loss: 1.5060362815856934\n",
      "Epoch: 11 Step: 3975 Valid loss: 1.502082347869873\n",
      "Epoch: 11 Step: 4000 Valid loss: 1.499100685119629\n",
      "Epoch: 11 Step: 4025 Valid loss: 1.5017919540405273\n",
      "Epoch: 11 Step: 4050 Valid loss: 1.5020904541015625\n",
      "Epoch: 11 Step: 4075 Valid loss: 1.496515154838562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Step: 4100 Valid loss: 1.4950679540634155\n",
      "Epoch: 11 Step: 4125 Valid loss: 1.4969258308410645\n",
      "Epoch: 11 Step: 4150 Valid loss: 1.497747778892517\n",
      "Epoch: 11 Step: 4175 Valid loss: 1.4951013326644897\n",
      "Epoch: 11 Step: 4200 Valid loss: 1.4957243204116821\n",
      "Epoch: 12 Step: 4225 Valid loss: 1.494715690612793\n",
      "Epoch: 12 Step: 4250 Valid loss: 1.4937050342559814\n",
      "Epoch: 12 Step: 4275 Valid loss: 1.4934042692184448\n",
      "Epoch: 12 Step: 4300 Valid loss: 1.4896453619003296\n",
      "Epoch: 12 Step: 4325 Valid loss: 1.4863756895065308\n",
      "Epoch: 12 Step: 4350 Valid loss: 1.4871530532836914\n",
      "Epoch: 12 Step: 4375 Valid loss: 1.4866877794265747\n",
      "Epoch: 12 Step: 4400 Valid loss: 1.485019564628601\n",
      "Epoch: 12 Step: 4425 Valid loss: 1.4820102453231812\n",
      "Epoch: 12 Step: 4450 Valid loss: 1.485376000404358\n",
      "Epoch: 12 Step: 4475 Valid loss: 1.483302354812622\n",
      "Epoch: 12 Step: 4500 Valid loss: 1.480330467224121\n",
      "Epoch: 12 Step: 4525 Valid loss: 1.4836143255233765\n",
      "Epoch: 12 Step: 4550 Valid loss: 1.4793413877487183\n",
      "Epoch: 12 Step: 4575 Valid loss: 1.4844441413879395\n",
      "Epoch: 13 Step: 4600 Valid loss: 1.4806468486785889\n",
      "Epoch: 13 Step: 4625 Valid loss: 1.4812378883361816\n",
      "Epoch: 13 Step: 4650 Valid loss: 1.4795467853546143\n",
      "Epoch: 13 Step: 4675 Valid loss: 1.475335717201233\n",
      "Epoch: 13 Step: 4700 Valid loss: 1.4770996570587158\n",
      "Epoch: 13 Step: 4725 Valid loss: 1.4729483127593994\n",
      "Epoch: 13 Step: 4750 Valid loss: 1.4740660190582275\n",
      "Epoch: 13 Step: 4775 Valid loss: 1.4763996601104736\n",
      "Epoch: 13 Step: 4800 Valid loss: 1.4734132289886475\n",
      "Epoch: 13 Step: 4825 Valid loss: 1.469456434249878\n",
      "Epoch: 13 Step: 4850 Valid loss: 1.4728424549102783\n",
      "Epoch: 13 Step: 4875 Valid loss: 1.4728633165359497\n",
      "Epoch: 13 Step: 4900 Valid loss: 1.4694103002548218\n",
      "Epoch: 13 Step: 4925 Valid loss: 1.4672147035598755\n",
      "Epoch: 13 Step: 4950 Valid loss: 1.47080397605896\n",
      "Epoch: 14 Step: 4975 Valid loss: 1.468325138092041\n",
      "Epoch: 14 Step: 5000 Valid loss: 1.4725375175476074\n",
      "Epoch: 14 Step: 5025 Valid loss: 1.4703755378723145\n",
      "Epoch: 14 Step: 5050 Valid loss: 1.4658747911453247\n",
      "Epoch: 14 Step: 5075 Valid loss: 1.4671716690063477\n",
      "Epoch: 14 Step: 5100 Valid loss: 1.4620877504348755\n",
      "Epoch: 14 Step: 5125 Valid loss: 1.4659416675567627\n",
      "Epoch: 14 Step: 5150 Valid loss: 1.4600125551223755\n",
      "Epoch: 14 Step: 5175 Valid loss: 1.4634392261505127\n",
      "Epoch: 14 Step: 5200 Valid loss: 1.4648911952972412\n",
      "Epoch: 14 Step: 5225 Valid loss: 1.4640201330184937\n",
      "Epoch: 14 Step: 5250 Valid loss: 1.4584084749221802\n",
      "Epoch: 14 Step: 5275 Valid loss: 1.4599542617797852\n",
      "Epoch: 14 Step: 5300 Valid loss: 1.462033748626709\n",
      "Epoch: 14 Step: 5325 Valid loss: 1.462461233139038\n",
      "Epoch: 15 Step: 5350 Valid loss: 1.46159029006958\n",
      "Epoch: 15 Step: 5375 Valid loss: 1.4607259035110474\n",
      "Epoch: 15 Step: 5400 Valid loss: 1.4567886590957642\n",
      "Epoch: 15 Step: 5425 Valid loss: 1.4555550813674927\n",
      "Epoch: 15 Step: 5450 Valid loss: 1.455887794494629\n",
      "Epoch: 15 Step: 5475 Valid loss: 1.4519624710083008\n",
      "Epoch: 15 Step: 5500 Valid loss: 1.4513335227966309\n",
      "Epoch: 15 Step: 5525 Valid loss: 1.4495189189910889\n",
      "Epoch: 15 Step: 5550 Valid loss: 1.4512006044387817\n",
      "Epoch: 15 Step: 5575 Valid loss: 1.4501794576644897\n",
      "Epoch: 15 Step: 5600 Valid loss: 1.4485962390899658\n",
      "Epoch: 15 Step: 5625 Valid loss: 1.449164628982544\n",
      "Epoch: 15 Step: 5650 Valid loss: 1.4460862874984741\n",
      "Epoch: 15 Step: 5675 Valid loss: 1.4482955932617188\n",
      "Epoch: 15 Step: 5700 Valid loss: 1.4490324258804321\n",
      "Epoch: 15 Step: 5725 Valid loss: 1.4463998079299927\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in generate_batches(train_data, batch_size,seq_len):\n",
    "        tracker+=1\n",
    "        \n",
    "        x = one_hot_encoder(x, num_char)\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model(inputs, hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(valid_data, batch_size, seq_len):\n",
    "                x = one_hot_encoder(x, num_char)\n",
    "        \n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                lstm_output, hidden = model(inputs, hidden)\n",
    "                val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {epoch+1} Step: {tracker} Valid loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'shakes.net'\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        # Encode raw letters with model\n",
    "        encoded_text = model.encoder[char]\n",
    "        \n",
    "        # set as numpy array for one hot encoding\n",
    "        # NOTE THE [[ ]] dimensions!!\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        \n",
    "        # One hot encoding\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        # Check for CPU\n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        \n",
    "        # Grab hidden states\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        \n",
    "        # Run model and get predicted output\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "        \n",
    "        # Convert lstm_out to probabilities\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            # move back to CPU to use with numpy\n",
    "            probs = probs.cpu()\n",
    "        \n",
    "        \n",
    "        # k determines how many characters to consider\n",
    "        # for our probability choice.\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "        \n",
    "        # Return k largest probabilities in tensor\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        \n",
    "        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        # Create array of probabilities\n",
    "        probs = probs.numpy().flatten()\n",
    "        \n",
    "        # Convert to probabilities per index\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "      \n",
    "    \n",
    "    # CHECK FOR GPU\n",
    "    if(model.use_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # begin output from initial seed\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    # intiate hidden state\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    # predict the next character for every character in seed\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    # add initial characters to output\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    # Now generate for size requested\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The of me, and his love\n",
      "    so treators, and shall the better some breath a done, to the service\n",
      "    to so so but the banition of the watch a storing to me to thee and the\n",
      "    service, and thou sense her any horse of their see of his service to the son that thou\n",
      "    take the strings this bear to-morrow till the seem and high as the better that the she say\n",
      "    the bastles, that which is a care, the countly the boy\n",
      "    to this son and heaven, and there is the string to the best on the strate.\n",
      "    If they will seem to the bear too, and to the some tongue,\n",
      "    the she are and hours and him, and the strean of this station, with\n",
      "    the strange of his true the child of her to my live, and the beand of this sound\n",
      "    the better.\n",
      "    There was this the canst the state of my leaving of\n",
      "    As so this truth, the can the stating to me\n",
      "    That shall be a money to have the straine and she\n",
      "    shall she take this stand to the bounce and so breath.\n",
      "    The will this thing on the comes of his father.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The ', k=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
